{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Point transformer denoising benchmark implementation\n\n## Evaluating PointTransformer on a PointCleanNet benchmark","metadata":{"id":"U7szutsXmEao","papermill":{"duration":0.032525,"end_time":"2021-06-01T15:15:40.118655","exception":false,"start_time":"2021-06-01T15:15:40.08613","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"C4GutaJHmEat","papermill":{"duration":0.030874,"end_time":"2021-06-01T15:15:40.244977","exception":false,"start_time":"2021-06-01T15:15:40.214103","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","metadata":{"id":"Oi5gbToTf2Nb","papermill":{"duration":0.071341,"end_time":"2021-06-01T15:15:40.347525","exception":false,"start_time":"2021-06-01T15:15:40.276184","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:48:56.279352Z","iopub.execute_input":"2022-02-02T10:48:56.279967Z","iopub.status.idle":"2022-02-02T10:48:56.320103Z","shell.execute_reply.started":"2022-02-02T10:48:56.279869Z","shell.execute_reply":"2022-02-02T10:48:56.319349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -qq open3d==0.12.0","metadata":{"papermill":{"duration":79.573618,"end_time":"2021-06-01T15:16:59.952303","exception":false,"start_time":"2021-06-01T15:15:40.378685","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:48:56.321963Z","iopub.execute_input":"2022-02-02T10:48:56.322311Z","iopub.status.idle":"2022-02-02T10:49:02.545358Z","shell.execute_reply.started":"2022-02-02T10:48:56.322276Z","shell.execute_reply":"2022-02-02T10:49:02.544286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUDA, = !readlink /usr/local/cuda | sed -E 's/.*cuda-(\\w+)\\.(\\w+)/cu\\1\\2/'\n# !pip install -qq torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+{CUDA}.html\n# !pip install -qq torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+{CUDA}.html\n!pip install -qq torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+{CUDA}.html\n# !pip install -qq torch-geometric","metadata":{"papermill":{"duration":33.234954,"end_time":"2021-06-01T15:17:33.243664","exception":false,"start_time":"2021-06-01T15:17:00.00871","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:02.547385Z","iopub.execute_input":"2022-02-02T10:49:02.547761Z","iopub.status.idle":"2022-02-02T10:49:08.667861Z","shell.execute_reply.started":"2022-02-02T10:49:02.547719Z","shell.execute_reply":"2022-02-02T10:49:08.66679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -Uqq pytorch-lightning\n# !pip install -Uqq wandb","metadata":{"papermill":{"duration":15.122815,"end_time":"2021-06-01T15:17:48.498349","exception":false,"start_time":"2021-06-01T15:17:33.375534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:08.670783Z","iopub.execute_input":"2022-02-02T10:49:08.671179Z","iopub.status.idle":"2022-02-02T10:49:15.044892Z","shell.execute_reply.started":"2022-02-02T10:49:08.671144Z","shell.execute_reply":"2022-02-02T10:49:15.043888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -qq torchtyping","metadata":{"papermill":{"duration":0.06528,"end_time":"2021-06-01T15:17:33.3425","exception":false,"start_time":"2021-06-01T15:17:33.27722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:15.048145Z","iopub.execute_input":"2022-02-02T10:49:15.04842Z","iopub.status.idle":"2022-02-02T10:49:15.079219Z","shell.execute_reply.started":"2022-02-02T10:49:15.048387Z","shell.execute_reply":"2022-02-02T10:49:15.078505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nfrom torchvision import transforms\n\nfrom torch_cluster import fps\n\nimport pytorch_lightning as pl\n\nimport open3d as o3d","metadata":{"id":"KgLKXHDkmEau","papermill":{"duration":5.600465,"end_time":"2021-06-01T15:17:54.130586","exception":false,"start_time":"2021-06-01T15:17:48.530121","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:15.080354Z","iopub.execute_input":"2022-02-02T10:49:15.080721Z","iopub.status.idle":"2022-02-02T10:49:17.663578Z","shell.execute_reply.started":"2022-02-02T10:49:15.080683Z","shell.execute_reply":"2022-02-02T10:49:17.662602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.__version__","metadata":{"papermill":{"duration":0.086884,"end_time":"2021-06-01T15:17:54.24834","exception":false,"start_time":"2021-06-01T15:17:54.161456","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:17.665106Z","iopub.execute_input":"2022-02-02T10:49:17.665459Z","iopub.status.idle":"2022-02-02T10:49:17.720642Z","shell.execute_reply.started":"2022-02-02T10:49:17.665404Z","shell.execute_reply":"2022-02-02T10:49:17.71978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: check if there's anything else to seed\nseed = 42\nrng = np.random.default_rng(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False","metadata":{"papermill":{"duration":0.0823,"end_time":"2021-06-01T15:17:54.362492","exception":false,"start_time":"2021-06-01T15:17:54.280192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:17.724176Z","iopub.execute_input":"2022-02-02T10:49:17.724446Z","iopub.status.idle":"2022-02-02T10:49:17.773715Z","shell.execute_reply.started":"2022-02-02T10:49:17.72442Z","shell.execute_reply":"2022-02-02T10:49:17.77283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data utilities","metadata":{"id":"2WAyEqzmmEau","papermill":{"duration":0.030835,"end_time":"2021-06-01T15:17:54.424496","exception":false,"start_time":"2021-06-01T15:17:54.393661","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Put path to data here\nPATH = Path(\"path/to/pointcleannetoutlierstestset/pointCleanNetOutliersTestSet/\")\n\nground_truth_filenames = glob.glob(str(PATH) + \"/*.outliers\")\nground_truth_filenames = list(map(lambda x: Path(x), ground_truth_filenames))\nlen(ground_truth_filenames)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:21.653615Z","iopub.execute_input":"2022-02-02T10:49:21.654071Z","iopub.status.idle":"2022-02-02T10:49:21.732165Z","shell.execute_reply.started":"2022-02-02T10:49:21.65403Z","shell.execute_reply":"2022-02-02T10:49:21.731297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sampler\n\nConverters:\n- PointCloudToTensor: point cloud to torch.Tensor\n\nTransforms:\n- PointCloudTranslate: translate point cloud by some value\n- PointCloudRotate: can be used to rotate around any given axis\n- PointCloudRotationalPerturbation\n- PointCloudJitter: clipped Gaussian(0, sigma^2) noise\n- PointCloudDropout\n- PointNormalize\n- **(NOT USED)** PointCloudScale: scale points by some value.\n\nOther:\n- PointShuffle","metadata":{"papermill":{"duration":0.030961,"end_time":"2021-06-01T15:17:54.486577","exception":false,"start_time":"2021-06-01T15:17:54.455616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Returns 3x3 rotation matrix that rotates by angle around axis\ndef get_rotation_matrix(angle, axis):\n    # Unit vector in axis direction\n    u = axis / np.linalg.norm(axis)\n\n    cross_prod_mat = np.array(\n        [[0.0, -u[2], u[1]],\n        [u[2], 0.0, -u[0]],\n        [-u[1], u[0], 0.0]]\n    )\n\n    cosval, sinval = np.cos(angle), np.sin(angle)\n    rot_matrix = torch.from_numpy(\n        cosval * np.eye(3)\n        + sinval * cross_prod_mat\n        + (1.0 - cosval) * np.outer(u, u)\n    )\n\n    return rot_matrix.float()","metadata":{"papermill":{"duration":0.082173,"end_time":"2021-06-01T15:17:54.60018","exception":false,"start_time":"2021-06-01T15:17:54.518007","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:22.618657Z","iopub.execute_input":"2022-02-02T10:49:22.619077Z","iopub.status.idle":"2022-02-02T10:49:22.676285Z","shell.execute_reply.started":"2022-02-02T10:49:22.619038Z","shell.execute_reply":"2022-02-02T10:49:22.675232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converters\nclass PointCloudToTensor:\n    def __call__(self, x):\n        return torch.from_numpy(x).float()\n\n\n# If not stated otherwise, the below transformations work with normals too\n# Transforms\nclass PointCloudScale:\n    def __init__(self, lo=0.8, hi=1.25):\n        self.lo, self.hi = lo, hi\n\n    def __call__(self, points):\n        scale_by = rng.uniform(self.lo, self.hi)\n        points[:, 0:3] *= scale_by\n        return points\n\n    \nclass PointCloudTranslate:\n    def __init__(self, translate_range=0.1):\n        self.translate_range = translate_range\n\n    def __call__(self, points):\n        translate_by = rng.uniform(-self.translate_range, self.translate_range)\n        points[:, 0:3] += translate_by\n        return points\n\n\nclass PointCloudRotate:\n    def __init__(self, axis=np.array([0.0, 0.0, 1.0])):\n        self.axis = axis\n\n    def __call__(self, points):\n        rotation_angle = rng.uniform() * 2 * np.pi\n        rotation_matrix = get_rotation_matrix(rotation_angle, self.axis)\n\n        has_normals = points.shape[1] > 3\n        if not has_normals:\n            return torch.matmul(points, rotation_matrix.t())\n        else:\n            pc_xyz = points[:, 0:3]\n            pc_normals = points[:, 3:]\n            points[:, 0:3] = torch.matmul(pc_xyz, rotation_matrix.t())\n            points[:, 3:] = torch.matmul(pc_normals, rotation_matrix.t())\n\n            return points\n\n\nclass PointCloudRotationalPerturbation:\n    def __init__(self, angle_sigma=0.06, angle_clip=0.18):\n        self.angle_sigma, self.angle_clip = angle_sigma, angle_clip\n\n    def _get_angles(self):\n        angles = np.clip(\n            self.angle_sigma * rng.randn(3), -self.angle_clip, self.angle_clip\n        )\n\n        return angles\n\n    def __call__(self, points):\n        angles = self._get_angles()\n        Rx = get_rotation_matrix(angles[0], np.array([1.0, 0.0, 0.0]))\n        Ry = get_rotation_matrix(angles[1], np.array([0.0, 1.0, 0.0]))\n        Rz = get_rotation_matrix(angles[2], np.array([0.0, 0.0, 1.0]))\n\n        # Combined rotation matrix\n        rotation_matrix = torch.matmul(torch.matmul(Rz, Ry), Rx)\n\n        has_normals = points.shape[1] > 3\n        if not has_normals:\n            return torch.matmul(points, rotation_matrix.t())\n        else:\n            pc_xyz = points[:, 0:3]\n            pc_normals = points[:, 3:]\n            points[:, 0:3] = torch.matmul(pc_xyz, rotation_matrix.t())\n            points[:, 3:] = torch.matmul(pc_normals, rotation_matrix.t())\n\n            return points\n        \n\nclass PointCloudJitter:\n    def __init__(self, std=0.01, clip=0.05):\n        self.std, self.clip = std, clip\n\n    def __call__(self, points):\n        jittered_data = (\n            torch.normal(mean=0.0, std=self.std, size=(points.size(0), 3))\n            .clamp_(-self.clip, self.clip)\n        )\n        points[:, 0:3] += jittered_data\n        return points\n\n\n# TODO: rewrite this in torch\nclass PointCloudDropout:\n    def __init__(self, max_dropout_ratio=0.875):\n        assert max_dropout_ratio >= 0 and max_dropout_ratio < 1\n        self.max_dropout_ratio = max_dropout_ratio\n\n    def __call__(self, points):\n        dropout_ratio = rng.random() * self.max_dropout_ratio  # 0~0.875\n        drop_idx = np.where(rng.random((points.shape[0])) <= dropout_ratio)[0]\n        if len(drop_idx) > 0:\n            random_point_idx = rng.choice(list(set(range(points.shape[0])) - set(drop_idx.tolist())), size=1)[0]\n            points[drop_idx, :] = points[random_point_idx].clone()  # set to the random point\n\n        return points\n\n\nclass PointCloudNormalize:\n    def __call__(self, x):\n        x -= torch.mean(x, dim=0)\n        return x / torch.max(x.norm(dim=1))\n\n    \n# Other\nclass PointCloudShuffle:\n    def __call__(self, points):\n        return points[torch.randperm(points.shape[0]), :]","metadata":{"papermill":{"duration":0.106561,"end_time":"2021-06-01T15:17:54.737763","exception":false,"start_time":"2021-06-01T15:17:54.631202","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:22.908781Z","iopub.execute_input":"2022-02-02T10:49:22.909244Z","iopub.status.idle":"2022-02-02T10:49:22.991206Z","shell.execute_reply.started":"2022-02-02T10:49:22.909203Z","shell.execute_reply":"2022-02-02T10:49:22.990094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_POINTS = 2048\n\n# - Uncomment the transformations that you want to use\n# - Be careful when using PointCloudDropout, e.g. make \n#   sure to select a reasonable dropout ratio\n# - Shuffle does not really do anything because PointTransformer\n#   is invariant to reordering of the points\ntrain_transforms = transforms.Compose([\n    PointCloudToTensor(),\n#     PointCloudTranslate(),\n#     PointCloudRotate(),\n#     PointCloudRotationalPerturbation(),\n#     PointCloudJitter(),\n#     PointCloudDropout(), # dropout\n#     PointCloudShuffle(), # shuffling points\n    PointCloudNormalize(),\n])\n\nvalid_transforms = transforms.Compose([\n    PointCloudToTensor(),\n    PointCloudNormalize(),\n])","metadata":{"papermill":{"duration":0.129979,"end_time":"2021-06-01T15:17:55.111344","exception":false,"start_time":"2021-06-01T15:17:54.981365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:23.342132Z","iopub.execute_input":"2022-02-02T10:49:23.342439Z","iopub.status.idle":"2022-02-02T10:49:23.392067Z","shell.execute_reply.started":"2022-02-02T10:49:23.342409Z","shell.execute_reply":"2022-02-02T10:49:23.391262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data","metadata":{"id":"1Jty4tQwmEaw","papermill":{"duration":0.05104,"end_time":"2021-06-01T15:17:55.212889","exception":false,"start_time":"2021-06-01T15:17:55.161849","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import plotly.express as px\n\n\nDISTINCT_LABELS = [\"object\", \"noise\"]\n\ndef pc_show(item, is_noise, category):\n    x, y, z = [item[:, i] for i in range(3)]\n    labels = [DISTINCT_LABELS[point] for point in is_noise]\n    print(labels[:10])\n\n    df = pd.DataFrame(dict(\n        x=x,\n        y=z, # axes are somehow mixed up, so we have to swap these two\n        z=y,\n        is_noise=labels,\n        size=[2] * len(labels),\n    ))\n    \n    print(category)\n\n    color_discrete_map = dict(zip(DISTINCT_LABELS, px.colors.sequential.Turbo))\n\n    fig = px.scatter_3d(\n        df, x=\"x\", y=\"y\", z=\"z\", color=\"is_noise\", size=\"size\",\n        opacity=0.0,\n        size_max=2,\n        color_discrete_map=color_discrete_map,\n        category_orders=dict(is_noise=DISTINCT_LABELS)\n    )\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:23.777991Z","iopub.execute_input":"2022-02-02T10:49:23.778324Z","iopub.status.idle":"2022-02-02T10:49:25.187643Z","shell.execute_reply.started":"2022-02-02T10:49:23.778293Z","shell.execute_reply":"2022-02-02T10:49:25.186782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# NUM_POINTS = 2048\n\n# choice = rng.choice(pts.shape[0], NUM_POINTS, replace=pts.shape[0] < NUM_POINTS)\n\n# pc_show(pts[choice], labels[choice], fn.stem)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.189715Z","iopub.execute_input":"2022-02-02T10:49:25.190194Z","iopub.status.idle":"2022-02-02T10:49:25.246328Z","shell.execute_reply.started":"2022-02-02T10:49:25.190156Z","shell.execute_reply":"2022-02-02T10:49:25.245447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DISTINCT_LABELS = [\"object\", \"noise\"]\n\ndef pc_show_matplotlib(item, is_noise, category, with_noise=True):\n    fig = plt.figure(figsize=(30, 30))\n    ax = fig.add_subplot(projection=\"3d\")\n\n    markers = [\"o\"]\n    if with_noise:\n        markers.append(\"^\")\n    for i, m in enumerate(markers):\n        indices = (is_noise == i).nonzero()[0]\n        xs = item[indices, 0]\n        ys = item[indices, 2]\n        zs = item[indices, 1]\n        ax.scatter(-xs, ys, zs, marker=m)\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_zlabel(\"z\")\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.248559Z","iopub.execute_input":"2022-02-02T10:49:25.248981Z","iopub.status.idle":"2022-02-02T10:49:25.309642Z","shell.execute_reply.started":"2022-02-02T10:49:25.248921Z","shell.execute_reply":"2022-02-02T10:49:25.308714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# choice = rng.choice(xyz.shape[0], NUM_POINTS, replace=xyz.shape[0] < NUM_POINTS)\n\n# pc_show_matplotlib(xyz, labels, fn.stem, False)\n# pc_show_matplotlib(xyz[choice], labels[choice], fn.stem, False)\n# pc_show_matplotlib(xyz[choice], labels[choice], fn.stem, True)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.311333Z","iopub.execute_input":"2022-02-02T10:49:25.311756Z","iopub.status.idle":"2022-02-02T10:49:25.368133Z","shell.execute_reply.started":"2022-02-02T10:49:25.311717Z","shell.execute_reply":"2022-02-02T10:49:25.367212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_object(pc, labels):\n    pts = np.copy(pc)\n    labels = np.copy(labels)\n\n    split_pcs, split_labels = [], []\n    for i in range(1, pts.shape[0] // NUM_POINTS + 1):\n        choice = rng.choice(pts.shape[0], NUM_POINTS, replace=pts.shape[0] < NUM_POINTS)\n        \n        new_pc = pts[choice]\n        new_labels = labels[choice]\n        split_pcs.append(new_pc)\n        split_labels.append(new_labels)\n\n        mask = np.ones(pts.shape[0], dtype=bool)\n        mask[choice] = False\n        pts = pts[mask]\n        labels = labels[mask]\n    \n    return split_pcs, split_labels\n\n# Test\n\n# split_pcs, split_labels = split_object(xyz, labels)\n# len(split_pcs), len(split_labels), xyz.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.369497Z","iopub.execute_input":"2022-02-02T10:49:25.370086Z","iopub.status.idle":"2022-02-02T10:49:25.430912Z","shell.execute_reply.started":"2022-02-02T10:49:25.370049Z","shell.execute_reply":"2022-02-02T10:49:25.429941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# import random\n\n# to_plot = []\n# for i in range(10):\n#     obj = random.choice(list(zip(split_pcs, split_labels)))\n#     to_plot.append(obj)\n\n# for item in to_plot:\n#     pc_show_matplotlib(item[0], item[1], fn.stem, True)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.43223Z","iopub.execute_input":"2022-02-02T10:49:25.432725Z","iopub.status.idle":"2022-02-02T10:49:25.491295Z","shell.execute_reply.started":"2022-02-02T10:49:25.432681Z","shell.execute_reply":"2022-02-02T10:49:25.490365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointCleanNetDataset(torch.utils.data.Dataset):\n\n    def __init__(self, ground_truth_filenames, transforms, subset_ratio=None):\n\n        self.ground_truth_filenames = ground_truth_filenames\n        if subset_ratio is not None:\n            self.ground_truth_filenames = self.ground_truth_filenames[:int(len(self.ground_truth_filenames) * subset_ratio)]\n\n        self.transforms = transforms\n\n        self.classes = { 0: \"object\", 1: \"noise\" }\n\n        # Prepare data\n        self._prepare_pcs_and_labels()\n        print(len(self.pcs))\n        print(len(self.labels))\n        print(len(self.categories))\n        print(list(self.full_objects.keys())[:10])\n\n    def _prepare_pcs_and_labels(self):\n        self.pcs, self.labels, self.categories = [], [], []\n        self.full_objects = {}\n\n        for gr_truth_fn in tqdm(self.ground_truth_filenames):\n            category = gr_truth_fn.stem\n            pc = o3d.io.read_point_cloud(str(gr_truth_fn)[:-9] + \".xyz\")\n            pc = np.asarray(pc.points)\n            segm_labels = np.loadtxt(str(gr_truth_fn)).astype(np.int32)\n            \n            self.full_objects[category] = { \"pc\": pc, \"labels\": segm_labels }\n            \n            split_pcs, split_labels = split_object(pc, segm_labels)\n            self.pcs.extend(split_pcs)\n            self.labels.extend(split_labels)\n            self.categories.extend([category] * len(split_pcs))\n\n\n    def __len__(self):\n        return len(self.pcs)\n\n    def __getitem__(self, idx):\n        pc = self.pcs[idx]\n        x = self.transforms(pc)\n        \n        y = self.labels[idx]\n        category = self.categories[idx]\n\n        return x, y, category","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.633227Z","iopub.execute_input":"2022-02-02T10:49:25.633588Z","iopub.status.idle":"2022-02-02T10:49:25.69786Z","shell.execute_reply.started":"2022-02-02T10:49:25.633555Z","shell.execute_reply":"2022-02-02T10:49:25.696986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# ds = PointCleanNetDataset(ground_truth_filenames, None)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:49:25.969Z","iopub.execute_input":"2022-02-02T10:49:25.969265Z","iopub.status.idle":"2022-02-02T10:49:26.026585Z","shell.execute_reply.started":"2022-02-02T10:49:25.969239Z","shell.execute_reply":"2022-02-02T10:49:26.025676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing as mp\n\nclass PointCleanNetDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=16, num_workers=mp.cpu_count()):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def setup(self, subset_ratio=None, stage=None):\n        if stage == \"test\" or stage is None:\n            self.test_dset = PointCleanNetDataset(ground_truth_filenames, valid_transforms, subset_ratio)\n    \n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.test_dset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            drop_last=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )","metadata":{"papermill":{"duration":0.088634,"end_time":"2021-06-01T15:17:55.355387","exception":false,"start_time":"2021-06-01T15:17:55.266753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:26.325313Z","iopub.execute_input":"2022-02-02T10:49:26.325655Z","iopub.status.idle":"2022-02-02T10:49:26.386663Z","shell.execute_reply.started":"2022-02-02T10:49:26.325604Z","shell.execute_reply":"2022-02-02T10:49:26.385694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BS = 32\ndm = PointCleanNetDataModule(batch_size=BS)\ndm.setup()\n# dm.setup(subset_ratio=0.1) # use this for prototyping as it loads faster","metadata":{"papermill":{"duration":9.141797,"end_time":"2021-06-01T15:18:04.529464","exception":false,"start_time":"2021-06-01T15:17:55.387667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:49:26.718085Z","iopub.execute_input":"2022-02-02T10:49:26.718415Z","iopub.status.idle":"2022-02-02T10:50:35.21558Z","shell.execute_reply.started":"2022-02-02T10:49:26.718381Z","shell.execute_reply":"2022-02-02T10:50:35.214683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dm.test_dset)","metadata":{"papermill":{"duration":0.083533,"end_time":"2021-06-01T15:18:04.644918","exception":false,"start_time":"2021-06-01T15:18:04.561385","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.217306Z","iopub.execute_input":"2022-02-02T10:50:35.217907Z","iopub.status.idle":"2022-02-02T10:50:35.280259Z","shell.execute_reply.started":"2022-02-02T10:50:35.217866Z","shell.execute_reply":"2022-02-02T10:50:35.279276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"kghJ1M4NmEa0","papermill":{"duration":0.030982,"end_time":"2021-06-01T15:18:04.707389","exception":false,"start_time":"2021-06-01T15:18:04.676407","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The essence of this notebook is here","metadata":{}},{"cell_type":"code","source":"def get_neighbours(features, idx):\n    \"\"\"\n    Input:\n        features: input points data, [B, N, C]\n        idx: neighbour index data, [B, N, K]\n    Return:\n        new_points:, indexed points data, [B, N, K, C]\n    \"\"\"\n    \n    raw_size = idx.size()\n    \n    idx = idx.reshape(raw_size[0], -1)\n    idx = idx[..., None]\n    idx = idx.expand(-1, -1, features.shape[-1])\n    \n    res = features.gather(dim=1, index=idx)\n    res = res.reshape(*raw_size, -1)\n    \n    return res","metadata":{"papermill":{"duration":0.08232,"end_time":"2021-06-01T15:18:04.823122","exception":false,"start_time":"2021-06-01T15:18:04.740802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.282536Z","iopub.execute_input":"2022-02-02T10:50:35.282923Z","iopub.status.idle":"2022-02-02T10:50:35.342041Z","shell.execute_reply.started":"2022-02-02T10:50:35.282876Z","shell.execute_reply":"2022-02-02T10:50:35.341168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransitionDownBlock(nn.Module):\n\n    def __init__(self, in_dims, out_dims, num_neighbours=16, sampling_ratio=0.25):\n        super().__init__()\n        \n        self.num_neighbours = num_neighbours\n        self.sampling_ratio = sampling_ratio\n        \n        self.mlp = nn.Sequential(\n            nn.Conv1d(in_dims, out_dims, kernel_size=1, bias=False), # Should be the same as Linear since dims are transposed\n            nn.BatchNorm1d(out_dims),\n            nn.ReLU(),\n        )\n\n    def forward(self, x: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n        \n        # Flatten pos from [B, N(=num points), 3]\n        # to [B * N, 3]\n        tmp = pos.reshape((-1, pos.shape[-1]))\n        \n        # Create the tensor that will tell which point belongs to\n        # which batch element\n#         print(pos.shape)\n        batch = torch.arange(pos.shape[0]).cuda()\n        batch = torch.repeat_interleave(batch, repeats=pos.shape[1], dim=0)\n\n        # Get indices of sampled points in tmp\n        indices = fps(tmp, batch, ratio=self.sampling_ratio, random_start=True)\n        \n        # Get pos_sampled from tmp and index,\n        # shape is [B, int(N * self.sampling_ratio), 3]\n        pos_sampled = tmp[indices].reshape((pos.shape[0], -1, pos.shape[-1]))        \n        \n        \n        # KNN ------------------------------\n        # Get vector lengths using 2-norm\n        rel_dist = (pos_sampled[:, :, None, :] - pos[:, None, :, :]).norm(dim=-1)\n\n        # Get indices of k-nearest neighbours\n        num_points = x.shape[1]\n        _, neighbour_indices = rel_dist.topk(min(num_points, self.num_neighbours), largest=False)\n        \n        \n        # MLP -----------------------------\n        # Transforms input features\n        x = self.mlp(x.transpose(1, 2)).transpose(1, 2) # [B, N, out_dims]\n        \n        # Get only the neighbours\n        x_sampled = get_neighbours(x, neighbour_indices) # [B, N_sampled, k, out_dims]\n\n        \n        # MAX POOLING ---------------------\n        # Selects max value for each dimension over all neighbours\n        x_sampled = torch.max(x_sampled, dim=2)[0] # # [B, N_sampled, out_dims]\n\n        return x_sampled, pos_sampled","metadata":{"papermill":{"duration":0.087308,"end_time":"2021-06-01T15:18:04.941828","exception":false,"start_time":"2021-06-01T15:18:04.85452","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.34497Z","iopub.execute_input":"2022-02-02T10:50:35.345276Z","iopub.status.idle":"2022-02-02T10:50:35.407722Z","shell.execute_reply.started":"2022-02-02T10:50:35.345248Z","shell.execute_reply":"2022-02-02T10:50:35.406908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# features = torch.randn((16, 16, 256))\n# pos = torch.randn((16, 16, 3))\n# lateral_pos = torch.randn((16, 64, 3))\n\n# rel_dist = (lateral_pos[:, :, None, :] - pos[:, None, :, :]).norm(dim=-1)\n# weights, neighbour_indices = rel_dist.topk(3, largest=False)\n\n# res = get_neighbours(features, neighbour_indices)\n# res.shape","metadata":{"papermill":{"duration":0.080313,"end_time":"2021-06-01T15:18:05.053717","exception":false,"start_time":"2021-06-01T15:18:04.973404","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.408895Z","iopub.execute_input":"2022-02-02T10:50:35.409235Z","iopub.status.idle":"2022-02-02T10:50:35.463651Z","shell.execute_reply.started":"2022-02-02T10:50:35.409201Z","shell.execute_reply":"2022-02-02T10:50:35.462836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# features = torch.arange(2*8*2).reshape(2, 8, 2)\n# features\n# pos = torch.randn((2, 8, 3))\n# lateral_pos = torch.randn((2, 32, 3))\n\n# print(features[0])\n# rel_dist = (lateral_pos[:, :, None, :] - pos[:, None, :, :]).norm(dim=-1)\n# weights, neighbour_indices = rel_dist.topk(3, largest=False)\n# print(neighbour_indices[0, :3])\n\n# res = get_neighbours(features, neighbour_indices)\n# res.shape, res[0, :3]","metadata":{"papermill":{"duration":0.079133,"end_time":"2021-06-01T15:18:05.16465","exception":false,"start_time":"2021-06-01T15:18:05.085517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.465006Z","iopub.execute_input":"2022-02-02T10:50:35.465449Z","iopub.status.idle":"2022-02-02T10:50:35.518255Z","shell.execute_reply.started":"2022-02-02T10:50:35.465406Z","shell.execute_reply":"2022-02-02T10:50:35.517461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransitionUpBlock(nn.Module):\n\n    def __init__(self, in_dims, out_dims):\n        super().__init__()\n\n        self.up_mlp = nn.Sequential(\n            nn.Conv1d(in_dims, out_dims, kernel_size=1, bias=False),\n            nn.BatchNorm1d(out_dims),\n            nn.ReLU()\n        )\n        self.lateral_mlp = nn.Sequential(\n            nn.Conv1d(out_dims, out_dims, kernel_size=1, bias=False),\n            nn.BatchNorm1d(out_dims),\n            nn.ReLU()\n        )\n\n    def forward(self, features, pos, lateral_features, lateral_pos):\n        \"\"\"\n            features: (B, N, in_channels) torch.Tensor\n            pos: (B, N, 3) torch.Tensor\n            lateral_features: (B, M, out_channels) torch.Tensor\n            lateral_pos: (B, M, 3) torch.Tensor\n        Note that N is smaller than M because this module upsamples features.\n        \"\"\"\n        \n        features = self.up_mlp(features.transpose(1, 2)).transpose(1, 2)\n        \n        # Find three nearest neighbours of lateral_pos in pos\n        rel_dist = (lateral_pos[:, :, None, :] - pos[:, None, :, :]).norm(dim=-1)\n        weights, neighbour_indices = rel_dist.topk(3, largest=False)\n        \n        # Interpolation weights\n        weights = 1.0 / (weights + 1e-8)\n        weights = weights / torch.sum(weights, dim=2, keepdim=True) # [B, M, 3]\n        \n        # Get triplets of vectors to interpolate from\n        interpolated_features = get_neighbours(features, neighbour_indices) # [B, M, 3, C]\n        # Do interpolation using weights from above\n        interpolated_features = torch.sum(interpolated_features * weights[..., None], dim=-2)\n        \n        lateral_features = self.lateral_mlp(lateral_features.transpose(1, 2)).transpose(1, 2)\n        \n        # Add interpolated features to features from before\n        out = interpolated_features + lateral_features\n        \n        return out, lateral_pos\n","metadata":{"papermill":{"duration":0.086227,"end_time":"2021-06-01T15:18:05.282465","exception":false,"start_time":"2021-06-01T15:18:05.196238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.520648Z","iopub.execute_input":"2022-02-02T10:50:35.520966Z","iopub.status.idle":"2022-02-02T10:50:35.578593Z","shell.execute_reply.started":"2022-02-02T10:50:35.520899Z","shell.execute_reply":"2022-02-02T10:50:35.57785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# features = torch.randn((16, 16, 256))\n# pos = torch.randn((16, 16, 3))\n# lateral_features = torch.randn((16, 64, 128))\n# lateral_pos = torch.randn((16, 64, 3))\n\n# up = TransitionUpBlock(in_channels=256, out_channels=128)\n\n# with torch.no_grad():\n#     feat, pos = up(features, pos, lateral_features, lateral_pos)\n#     print(feat.shape)\n#     print(pos.shape)","metadata":{"papermill":{"duration":0.079046,"end_time":"2021-06-01T15:18:05.393056","exception":false,"start_time":"2021-06-01T15:18:05.31401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.581064Z","iopub.execute_input":"2022-02-02T10:50:35.581418Z","iopub.status.idle":"2022-02-02T10:50:35.631978Z","shell.execute_reply.started":"2022-02-02T10:50:35.581381Z","shell.execute_reply":"2022-02-02T10:50:35.631154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointTransformerLayer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_neighbors,\n        pos_mlp_hidden_dim=64,\n        attn_mlp_hidden_mult=4, # This comes from initial Transformer paper\n        dropout=0,\n    ):\n        super().__init__()\n        self.num_neighbors = num_neighbors\n\n        self.to_queries = nn.Linear(dim, dim, bias=False) # phi\n        self.to_keys = nn.Linear(dim, dim, bias=False) # psi\n        self.to_values = nn.Linear(dim, dim, bias=False) # alpha\n\n        # theta\n        self.pos_mlp = nn.Sequential(\n            nn.Linear(3, pos_mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(pos_mlp_hidden_dim, dim)\n        )\n        \n        # gamma\n        self.to_attn_weights = nn.Sequential(\n            nn.Linear(dim, dim * attn_mlp_hidden_mult),\n            nn.ReLU(),\n            nn.Linear(dim * attn_mlp_hidden_mult, dim),\n        )\n        \n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n        num_points, num_neighbors = x.shape[1], self.num_neighbors\n        \n        # get nearest neighbour indices\n        rel_dist = (pos[:, :, None, :] - pos[:, None, :, :]).norm(dim=-1)\n        _, neighbour_indices = rel_dist.topk(min(num_neighbors, num_points), largest=False)\n\n        # get queries, keys, values,\n        # immediately leaving only the neighbouring points for k and v\n        q = self.to_queries(x)\n        k = get_neighbours(self.to_keys(x), neighbour_indices)\n        v = get_neighbours(self.to_values(x), neighbour_indices)\n\n        # use subtraction relation between queries and keys\n        qk_rel = q[:, :, None, :] - k\n\n        # calculate position embeddings\n        rel_pos_emb = self.pos_mlp(pos[:, :, None, :] - get_neighbours(pos, neighbour_indices))\n        rel_pos_emb = self.drop(rel_pos_emb)\n        \n        # add relative positional embeddings to values\n        v += rel_pos_emb\n\n        # use attention weights mlp, making sure to add relative positional embedding first\n        rel_pos_emb = self.to_attn_weights(qk_rel + rel_pos_emb)\n\n        # attention weights\n        rel_pos_emb = rel_pos_emb.softmax(dim=-2)\n\n        # aggregate\n        agg = torch.sum(torch.mul(rel_pos_emb, v), dim=-2)\n        \n        return agg","metadata":{"papermill":{"duration":0.087292,"end_time":"2021-06-01T15:18:05.51161","exception":false,"start_time":"2021-06-01T15:18:05.424318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.63372Z","iopub.execute_input":"2022-02-02T10:50:35.634088Z","iopub.status.idle":"2022-02-02T10:50:35.692805Z","shell.execute_reply.started":"2022-02-02T10:50:35.634034Z","shell.execute_reply":"2022-02-02T10:50:35.691911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointTransformerBlock(nn.Module):\n    def __init__(self, dim, hidden_dim, pos_mlp_hidden_dim=64, attn_mlp_hidden_mult=4, dropouts=None, num_neighbours=None):\n        super().__init__()\n        \n        self.fc_in = nn.Linear(dim, hidden_dim)\n        self.point_transformer_layer = PointTransformerLayer(\n            dim=hidden_dim,\n            num_neighbors=num_neighbours,\n            pos_mlp_hidden_dim=pos_mlp_hidden_dim,\n            attn_mlp_hidden_mult=attn_mlp_hidden_mult, # This comes from initial Transformer paper\n            dropout=dropouts[0] if dropouts else 0,\n        )\n        self.fc_out = nn.Linear(hidden_dim, dim)\n        \n        self.drop = nn.Dropout(dropouts[1] if dropouts else 0)\n    \n    def forward(self, x: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n        out = self.fc_in(x)\n        \n        out = self.point_transformer_layer(out, pos)\n        \n        out = self.fc_out(out)\n        \n        out = self.drop(out)\n        \n        # Residual\n        out = out + x\n        \n        return out","metadata":{"papermill":{"duration":0.082999,"end_time":"2021-06-01T15:18:05.626103","exception":false,"start_time":"2021-06-01T15:18:05.543104","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.694149Z","iopub.execute_input":"2022-02-02T10:50:35.694532Z","iopub.status.idle":"2022-02-02T10:50:35.750683Z","shell.execute_reply.started":"2022-02-02T10:50:35.694497Z","shell.execute_reply":"2022-02-02T10:50:35.749815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# from itertools import tee\n\n# def pairwise(iterable):\n#     \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n#     a, b = tee(iterable)\n#     next(b, None)\n#     return list(zip(a, b))","metadata":{"papermill":{"duration":0.086772,"end_time":"2021-06-01T15:18:05.74425","exception":false,"start_time":"2021-06-01T15:18:05.657478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.751832Z","iopub.execute_input":"2022-02-02T10:50:35.752183Z","iopub.status.idle":"2022-02-02T10:50:35.807724Z","shell.execute_reply.started":"2022-02-02T10:50:35.752146Z","shell.execute_reply":"2022-02-02T10:50:35.806762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointTransformerSegmentator(nn.Module):\n    def __init__(self, layer_config, num_neighbours=16, sampling_ratio=0.25, dropouts=None):\n        super().__init__()\n        assert dropouts is None or len(dropouts) == 3\n        \n        in_dims, *inner_dims, out_dims = layer_config\n        encoder_dims = pairwise(inner_dims)\n        decoder_dims = pairwise(inner_dims[::-1])\n        \n        self.encoder_transitions = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(in_dims, inner_dims[0]),\n                nn.ReLU(),\n                nn.Linear(inner_dims[0], inner_dims[0]),\n            ),\n        ] + [\n            TransitionDownBlock(\n                in_dims=in_dims,\n                out_dims=out_dims,\n                num_neighbours=num_neighbours,\n                sampling_ratio=sampling_ratio,\n            )\n            for in_dims, out_dims in encoder_dims\n        ])\n        self.encoder_transformers = nn.ModuleList([\n            PointTransformerBlock(\n                dim=dims,\n                hidden_dim=dims,\n                pos_mlp_hidden_dim=dims,\n                attn_mlp_hidden_mult=4,\n                num_neighbours=num_neighbours,\n                dropouts=dropouts[:-1] if dropouts else None,\n            )\n            for dims in inner_dims\n        ])\n        \n        self.decoder_transitions = nn.ModuleList([\n            nn.Linear(inner_dims[-1], inner_dims[-1]),\n        ] + [\n            TransitionUpBlock(\n                in_dims=in_dims,\n                out_dims=out_dims,\n            )\n            for in_dims, out_dims in decoder_dims\n        ])\n        self.decoder_transformers = nn.ModuleList([\n            PointTransformerBlock(\n                dim=dims,\n                hidden_dim=dims,\n                pos_mlp_hidden_dim=dims,\n                attn_mlp_hidden_mult=4,\n                num_neighbours=num_neighbours,\n                dropouts=dropouts[:-1] if dropouts else None,\n            )\n            for dims in inner_dims[::-1]\n        ])\n        \n        self.drop = nn.Dropout(dropouts[-1] if dropouts else 0)\n        \n        self.fc_out = nn.Sequential(\n            nn.Linear(inner_dims[0], inner_dims[0]),\n            nn.ReLU(),\n            nn.Linear(inner_dims[0], out_dims),\n        )\n    \n    def forward(self, pos: torch.Tensor) -> torch.Tensor:\n        lateral_features = []\n        lateral_pos = []\n\n        for i, (trans_down, transformer) in enumerate(zip(self.encoder_transitions, self.encoder_transformers)):\n            if i == 0:\n                features = trans_down(pos)\n                features = transformer(features, pos)\n            else:\n                features, pos = trans_down(features, pos)\n                features = transformer(features, pos)\n\n            if i < len(self.encoder_transitions) - 1:\n                lateral_features.append(features)\n                lateral_pos.append(pos)\n\n        for i, (trans_up, transformer) in enumerate(zip(self.decoder_transitions, self.decoder_transformers)):\n            if i == 0:\n                features = trans_up(features)\n                features = transformer(features, pos)\n            else:\n                features, pos = trans_up(features, pos, lateral_features[-i], lateral_pos[-i])\n                features = transformer(features, pos)\n\n        out = self.drop(features)\n\n        out = self.fc_out(out)\n        \n        return out","metadata":{"papermill":{"duration":0.152234,"end_time":"2021-06-01T15:18:05.964126","exception":false,"start_time":"2021-06-01T15:18:05.811892","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.810615Z","iopub.execute_input":"2022-02-02T10:50:35.811199Z","iopub.status.idle":"2022-02-02T10:50:35.87924Z","shell.execute_reply.started":"2022-02-02T10:50:35.811161Z","shell.execute_reply":"2022-02-02T10:50:35.878299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{"id":"XbK48S6wmEa4","papermill":{"duration":0.05087,"end_time":"2021-06-01T15:18:06.06558","exception":false,"start_time":"2021-06-01T15:18:06.01471","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass CMCounts:\n    tp: int = 0\n    fp: int = 0\n    fn: int = 0\n    tn: int = 0\n    \n    # two tensors\n    @classmethod\n    def from_tensors(cls, target, preds):\n        tp, fp, fn, tn = torch.dstack((\n            preds & target > 0,\n            preds > target,\n            preds < target,\n            preds | target == 0,\n        )).sum((0, 1))\n        return cls(*[x.item() for x in [tp, fp, fn, tn]])\n        \n        \n    @property\n    def f1(self):\n        return self.tp / (self.tp + 0.5 * (self.fp + self.fn))\n    \n    @property\n    def f2(self):\n        return self.f_beta(2)\n    \n    def f_beta(self, beta):\n        return (1 + beta ** 2) * self.tp / ((1 + beta ** 2) * self.tp + (beta ** 2) * self.fn + self.fp)\n    \n    @property\n    def accuracy(self):\n        return (self.tp + self.tn) / (self.tp + self.fp + self.fn + self.tn)\n    \n    @property\n    def balanced_accuracy(self):\n        true_positive_rate = self.tp / (self.tp + self.fn)\n        true_negative_rate = self.tn / (self.tn + self.fp)\n        return (true_positive_rate + true_negative_rate) / 2\n    \n    def __add__(self, other):\n        return CMCounts(\n            tp=self.tp + other.tp,\n            fp=self.fp + other.fp,\n            fn=self.fn + other.fn,\n            tn=self.tn + other.tn,\n        )\n    \n    def __radd__(self, other):\n        return self if other == 0 else self + other","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:50:35.880735Z","iopub.execute_input":"2022-02-02T10:50:35.881206Z","iopub.status.idle":"2022-02-02T10:50:35.946995Z","shell.execute_reply.started":"2022-02-02T10:50:35.881168Z","shell.execute_reply":"2022-02-02T10:50:35.946061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationTask(pl.LightningModule):\n    def __init__(self, model, max_lr, epochs, steps_per_epoch, num_classes=2):\n        super().__init__()\n        self.save_hyperparameters(\"max_lr\", \"epochs\")\n        self.steps_per_epoch = steps_per_epoch\n        self.model = model\n        self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n        \n    def forward(self, x):\n        return self.model(x)\n\n\n    def _shared_step(self, batch, prefix):\n        x, y, _ = batch\n        logits = self.model(x).squeeze()\n        loss = self.loss(logits, y.float())\n        \n        y_preds = (logits >= 0.0).long() # since we do not call sigmoid\n        \n        cm_counts = CMCounts.from_tensors(y.flatten(), y_preds.flatten())\n\n        self.log(f\"{prefix}_loss\", loss, on_step=False, on_epoch=True)\n\n        return { \"loss\": loss, \"f1\": cm_counts }\n    \n    \n    # TODO: this is hard-coded, should be automated\n    def on_train_start(self):\n        self.logger.log_hyperparams({\n            \"bs\": BS, \"num_points\": NUM_POINTS,\n            \"max_lr\": self.hparams.max_lr, \"epochs\": self.hparams.epochs,\n            \"optimizer\": \"AdamW(wd=1e-2)\", \"scheduler\": \"OneCycleLR\",\n        })\n    \n    \n    def training_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"train\")\n\n\n    def training_epoch_end(self, outs):\n        cm_count_total = sum(map(lambda x: x[\"f1\"], outs))\n        self.log(\"train_f1\", cm_count_total.f1)\n\n\n    def validation_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"valid\")\n\n    \n    def validation_epoch_end(self, outs):\n        cm_count_total = sum(map(lambda x: x[\"f1\"], outs))\n        self.log(\"valid_f1\", cm_count_total.f1, )\n\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), weight_decay=1e-2)\n        lr_scheduler = optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.hparams.max_lr,\n            steps_per_epoch=self.steps_per_epoch,\n            epochs=self.hparams.epochs\n        )\n        lr_dict = { \"scheduler\": lr_scheduler, \"interval\": \"step\" }\n        return [optimizer], [lr_dict]","metadata":{"papermill":{"duration":0.108275,"end_time":"2021-06-01T15:18:06.327837","exception":false,"start_time":"2021-06-01T15:18:06.219562","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T10:50:35.948449Z","iopub.execute_input":"2022-02-02T10:50:35.948828Z","iopub.status.idle":"2022-02-02T10:50:36.015838Z","shell.execute_reply.started":"2022-02-02T10:50:35.948788Z","shell.execute_reply":"2022-02-02T10:50:36.014999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_config = [3, 32, 64, 128, 256, 512, 1]\n\nmodel = PointTransformerSegmentator(\n    layer_config=layer_config,\n    num_neighbours=16,\n    sampling_ratio=0.25,\n    dropouts=[0.0, 0.0, 0.0]\n)\n\n# Put your path to weights here\nsegmentator = SegmentationTask.load_from_checkpoint(\n    \"path/to/weights.ckpt\",\n    model=model,\n    steps_per_epoch=len(dm.test_dataloader())\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:50:36.749527Z","iopub.execute_input":"2022-02-02T10:50:36.749895Z","iopub.status.idle":"2022-02-02T10:50:39.423774Z","shell.execute_reply.started":"2022-02-02T10:50:36.749856Z","shell.execute_reply":"2022-02-02T10:50:39.422885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ndef evaluate(model, dl, threshold_values=None, quiet=False):\n    \n    if threshold_values is None:\n        threshold_values = [0.5]\n\n    model.cuda()\n    model.eval()\n    \n    instances = sorted(list(set(dl.dataset.categories)))\n    cm_count_dict = { key: defaultdict(CMCounts) for key in threshold_values }\n    with torch.no_grad():\n        for x, y, cats in tqdm(dl, total=len(dl)):\n            x, y = x.cuda(), y.cuda()\n            logits = model(x).squeeze()\n            \n            for th in threshold_values:\n                segm_preds = (logits.sigmoid() >= th).long()\n\n                cats = np.array(cats)\n                for i, cat in enumerate(instances):\n                    mask = cats == cat\n                    cat_ys = y[mask].flatten()\n                    cat_y_preds = segm_preds[mask].flatten()\n                    cm_count_new = CMCounts.from_tensors(y[mask].flatten(), segm_preds[mask].flatten())\n                    cm_count_dict[th][cat] += cm_count_new\n                \n    cm_count_totals = {\n        th_value: sum(category_values.values()) for th_value, category_values in cm_count_dict.items()\n    }\n    if not quiet:\n        key = 0.5 if 0.5 in threshold_values else threshold_values[0]\n        print(f\"F1: {cm_count_totals[key].f1}\")\n        print(f\"F2: {cm_count_totals[key].f2}\")\n        print(f\"Acc: {cm_count_totals[key].accuracy}\")\n        print(f\"Balanced acc: {cm_count_totals[key].balanced_accuracy}\")\n    \n    return cm_count_totals\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:50:39.425175Z","iopub.execute_input":"2022-02-02T10:50:39.425588Z","iopub.status.idle":"2022-02-02T10:50:39.489802Z","shell.execute_reply.started":"2022-02-02T10:50:39.425549Z","shell.execute_reply":"2022-02-02T10:50:39.488839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\n# evaluate(segmentator, dm.test_dataloader());","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:50:39.49116Z","iopub.execute_input":"2022-02-02T10:50:39.491597Z","iopub.status.idle":"2022-02-02T10:51:43.535808Z","shell.execute_reply.started":"2022-02-02T10:50:39.491556Z","shell.execute_reply":"2022-02-02T10:51:43.534717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 0.05\nb = 1.0\nstep = 0.05\nthreshold_dict = {\n    \"threshold_values\": np.arange(a, b, step),\n    \"metric_values\": {\n        \"f1\": [],\n        \"f2\": [],\n        \"accuracy\": [],\n        \"balanced_accuracy\": [],\n    },\n}\nprint(threshold_dict[\"threshold_values\"])\n\ncount_totals = evaluate(segmentator, dm.test_dataloader(), threshold_values=threshold_dict[\"threshold_values\"], quiet=True)\nthreshold_dict[\"metric_values\"][\"f1\"] = [count_totals[th_value].f1 for th_value in count_totals]\nthreshold_dict[\"metric_values\"][\"f2\"] = [count_totals[th_value].f2 for th_value in count_totals]\nthreshold_dict[\"metric_values\"][\"accuracy\"] = [count_totals[th_value].accuracy for th_value in count_totals]\nthreshold_dict[\"metric_values\"][\"balanced_accuracy\"] = [count_totals[th_value].balanced_accuracy for th_value in count_totals]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:51:43.537773Z","iopub.execute_input":"2022-02-02T10:51:43.538341Z","iopub.status.idle":"2022-02-02T10:55:02.498758Z","shell.execute_reply.started":"2022-02-02T10:51:43.538288Z","shell.execute_reply":"2022-02-02T10:55:02.496606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, axes = plt.subplots(1, 4, sharey=True, figsize=(25, 5)) # uncomment this for a 1x4 grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\nbenchmark_values = [0.77, 0.86, None, None]\n\ntranslation_dict = { \"f1\": \"f1 vertis\", \"f2\": \"f2 vertis\", \"accuracy\": \"Tikslumas\", \"balanced_accuracy\": \"Subalansuotas tikslumas\" }\n\nx = threshold_dict[\"threshold_values\"]\nmax_value_indices = [x.index(max(x)) for _, x in threshold_dict[\"metric_values\"].items()]\nfor ax, metric_label, max_value_index, benchmark_value in zip(axes.flat, list(threshold_dict[\"metric_values\"].keys()), max_value_indices, benchmark_values):\n    ax.plot(x, threshold_dict[\"metric_values\"][metric_label], marker=\"o\", markevery=[max_value_index])\n    ax.set_xticks(np.arange(0.0, 1.0, 0.1))\n    ax.set_title(translation_dict[metric_label])\n    ax.set_xlabel(\"Slenksio vert\")\n    ax.annotate(\n        f\"Maksimumas take ({threshold_dict['threshold_values'][max_value_index]:.2f}; {threshold_dict['metric_values'][metric_label][max_value_index]:.4f})\",\n        (\n            threshold_dict[\"threshold_values\"][max_value_index] - 0.15,\n            threshold_dict[\"metric_values\"][metric_label][max_value_index] + 0.015\n        )\n    )\n    \n    if benchmark_value is not None:\n        ax.axhline(y=benchmark_value, color=\"grey\", linestyle=\"--\")\n        ax.annotate(\n        f\"PointCleanNet: {benchmark_value:.2f}\",\n        (\n            0.3,\n            benchmark_value + 0.015\n        )\n    )\n    \n    ax.set_ylim(0.5, 1)\n\n# Choose a path where you want to safe the figure\nplt.savefig(\"benchmark_comparison_2x2.png\", dpi=200)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:55:02.50506Z","iopub.execute_input":"2022-02-02T10:55:02.50717Z","iopub.status.idle":"2022-02-02T10:55:03.835186Z","shell.execute_reply.started":"2022-02-02T10:55:02.507083Z","shell.execute_reply":"2022-02-02T10:55:03.834334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what happens when maximizing f1\nf1_max_value_threshold = threshold_dict[\"threshold_values\"][max_value_indices[0]]\n\nevaluate(segmentator, dm.test_dataloader(), threshold_values=[f1_max_value_threshold]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what happens when maximizing f2\nf2_max_value_threshold = threshold_dict[\"threshold_values\"][max_value_indices[1]]\n\nevaluate(segmentator, dm.test_dataloader(), threshold_values=[f2_max_value_threshold]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}